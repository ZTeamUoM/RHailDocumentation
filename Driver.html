<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>RideSimulator.Driver API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>RideSimulator.Driver</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import simpy
import simpy
import logging
import numpy as np
# import tensorflow as tf
from RideSimulator.Trip import Trip
import RideSimulator.reward_parameters as rp

class Driver(object):
    &#34;&#34;&#34;
    The Driver class models the attributes and the behaviour of driver agents.

    :param env: simpy environment
    :param driver_id: driver id
    :param location: initial location of the driver when created
    :param spot_id: id of the nearest driver pool
    &#34;&#34;&#34;

    def __init__(self, env: simpy.Environment, driver_id: int, location: np.ndarray, spot_id: int):

        self.env = env
        self.id = driver_id
        self.location = location
        self.in_trip = False
        self.trip_duration = 0
        self.weekly_target = rp.base_reward_threshold
        self.reward_amount = rp.base_reward
        self.trip_reward_weight = rp.trip_reward_weights[self.id % rp.weights_length]
        self.weekly_reward_weight = rp.weekly_reward_weights[self.id % rp.weights_length]
        self.opportunity_cost_weight = rp.opportunity_cost_weights[self.id % rp.weights_length]
        self.action = env.process(self.run())
        self.weekly_trip_count = 0
        self.total_reward = 0
        self.weekly_reward_reached = False
        self.trip = None
        self.spot_id = spot_id
        self.state_history = []
        self.idle_time_begin = 0

    def run(self):
        while True:
            try:
                if self.in_trip:
                    yield self.env.timeout(self.trip_duration)
                    logging.debug(f&#34;Driver {self.id} finished trip&#34;)
                else:
                    yield self.env.timeout(float(&#39;inf&#39;))
            except simpy.Interrupt:
                continue

    def reset_weekly_info(self, new_target, new_reward):
        self.weekly_trip_count = 0
        self.weekly_reward_reached = False
        # self.total_reward = 0
        self.weekly_target = new_target
        self.reward_amount = new_reward

    def start_trip(self, duration: int, location: np.ndarray, trip: Trip):
        &#34;&#34;&#34;
        When the driver starts the trip, his &#39;total_trip_count&#39; and &#39;weekly_reward_reached&#39; values are updated.
        If the driver has achieved the weekly goal, an additional amount of reward is added.

        :param duration: trip duration
        :param location: drop location of the trip
        :param trip: trip object
        &#34;&#34;&#34;
        self.in_trip = True
        self.trip = trip
        self.trip_duration = duration
        self.weekly_trip_count += 1
        self.location = location
        add_reward = False
        logging.debug(f&#34;driver {self.id} location updated  to {self.location}&#34;)

        # Record rewards and distances.
        self.total_reward += trip.get_net_reward()

        if (not self.weekly_reward_reached) and (self.weekly_trip_count &gt;= self.weekly_target):
            self.weekly_reward_reached = True
            self.total_reward += self.reward_amount
            add_reward = True
            logging.debug(f&#34;driver {self.id} has achieved his weekly reward&#34;)
        self.action.interrupt()

        return add_reward

    def accept_ride(self, policy, observations) -&gt; bool:
        &#34;&#34;&#34;
        For now the driver randomly accepts or reject the trip.
        The reinforcement learning model will be exposed to this function and the state will be passed to make the
        decision.

        :return: the decision of the driver
        &#34;&#34;&#34;
        driver_policy = policy[self.id % len(policy)]
        # TODO - check before commit
        # previous reward shouldn&#39;t matter when decision making, so leave reward = 0?
        # observation_ts = ts.transition(np.array(observations, dtype=np.float32), reward=0.0, discount=1.0)
        # observation_ts = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]),
        #                              tf.convert_to_tensor(np.array([observations], dtype=np.float32), dtype=tf.float32))
        # logging.debug(observation_ts)
        # action = driver_policy.action(observation_ts)
        #print(&#34;obs&#34;, observations, &#34;act&#34;, action)

        if policy[0] == &#34;random&#34;:
            action = tf.random.uniform([1], 0, 2, dtype=tf.int32)

        elif policy[0] == &#34;positive&#34;:
            if observations[0] &lt; observations[1]:
                action = tf.random.uniform([1], 1, 2, dtype=tf.int32)
            else:
                action = tf.random.uniform([1], 0, 1, dtype=tf.int32)
        elif policy[0] == &#34;all&#34;:
            action = tf.random.uniform([1], 1, 2, dtype=tf.int32)
        else:
            if type(policy[0] == &#39;str&#39;):
                action = tf.random.uniform([1], 0, 2, dtype=tf.int32)

        return action

    def get_state_info(self) -&gt; dict:
        &#34;&#34;&#34;
        :return: the trip count and reward status of the driver
        &#34;&#34;&#34;
        return {
            &#34;goal_achieved&#34;: self.weekly_reward_reached,
            &#34;trip_count&#34;: self.weekly_trip_count
        }</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="RideSimulator.Driver.Driver"><code class="flex name class">
<span>class <span class="ident">Driver</span></span>
<span>(</span><span>env: simpy.core.Environment, driver_id: int, location: numpy.ndarray, spot_id: int)</span>
</code></dt>
<dd>
<div class="desc"><p>The Driver class models the attributes and the behaviour of driver agents.</p>
<p>:param env: simpy environment
:param driver_id: driver id
:param location: initial location of the driver when created
:param spot_id: id of the nearest driver pool</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Driver(object):
    &#34;&#34;&#34;
    The Driver class models the attributes and the behaviour of driver agents.

    :param env: simpy environment
    :param driver_id: driver id
    :param location: initial location of the driver when created
    :param spot_id: id of the nearest driver pool
    &#34;&#34;&#34;

    def __init__(self, env: simpy.Environment, driver_id: int, location: np.ndarray, spot_id: int):

        self.env = env
        self.id = driver_id
        self.location = location
        self.in_trip = False
        self.trip_duration = 0
        self.weekly_target = rp.base_reward_threshold
        self.reward_amount = rp.base_reward
        self.trip_reward_weight = rp.trip_reward_weights[self.id % rp.weights_length]
        self.weekly_reward_weight = rp.weekly_reward_weights[self.id % rp.weights_length]
        self.opportunity_cost_weight = rp.opportunity_cost_weights[self.id % rp.weights_length]
        self.action = env.process(self.run())
        self.weekly_trip_count = 0
        self.total_reward = 0
        self.weekly_reward_reached = False
        self.trip = None
        self.spot_id = spot_id
        self.state_history = []
        self.idle_time_begin = 0

    def run(self):
        while True:
            try:
                if self.in_trip:
                    yield self.env.timeout(self.trip_duration)
                    logging.debug(f&#34;Driver {self.id} finished trip&#34;)
                else:
                    yield self.env.timeout(float(&#39;inf&#39;))
            except simpy.Interrupt:
                continue

    def reset_weekly_info(self, new_target, new_reward):
        self.weekly_trip_count = 0
        self.weekly_reward_reached = False
        # self.total_reward = 0
        self.weekly_target = new_target
        self.reward_amount = new_reward

    def start_trip(self, duration: int, location: np.ndarray, trip: Trip):
        &#34;&#34;&#34;
        When the driver starts the trip, his &#39;total_trip_count&#39; and &#39;weekly_reward_reached&#39; values are updated.
        If the driver has achieved the weekly goal, an additional amount of reward is added.

        :param duration: trip duration
        :param location: drop location of the trip
        :param trip: trip object
        &#34;&#34;&#34;
        self.in_trip = True
        self.trip = trip
        self.trip_duration = duration
        self.weekly_trip_count += 1
        self.location = location
        add_reward = False
        logging.debug(f&#34;driver {self.id} location updated  to {self.location}&#34;)

        # Record rewards and distances.
        self.total_reward += trip.get_net_reward()

        if (not self.weekly_reward_reached) and (self.weekly_trip_count &gt;= self.weekly_target):
            self.weekly_reward_reached = True
            self.total_reward += self.reward_amount
            add_reward = True
            logging.debug(f&#34;driver {self.id} has achieved his weekly reward&#34;)
        self.action.interrupt()

        return add_reward

    def accept_ride(self, policy, observations) -&gt; bool:
        &#34;&#34;&#34;
        For now the driver randomly accepts or reject the trip.
        The reinforcement learning model will be exposed to this function and the state will be passed to make the
        decision.

        :return: the decision of the driver
        &#34;&#34;&#34;
        driver_policy = policy[self.id % len(policy)]
        # TODO - check before commit
        # previous reward shouldn&#39;t matter when decision making, so leave reward = 0?
        # observation_ts = ts.transition(np.array(observations, dtype=np.float32), reward=0.0, discount=1.0)
        # observation_ts = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]),
        #                              tf.convert_to_tensor(np.array([observations], dtype=np.float32), dtype=tf.float32))
        # logging.debug(observation_ts)
        # action = driver_policy.action(observation_ts)
        #print(&#34;obs&#34;, observations, &#34;act&#34;, action)

        if policy[0] == &#34;random&#34;:
            action = tf.random.uniform([1], 0, 2, dtype=tf.int32)

        elif policy[0] == &#34;positive&#34;:
            if observations[0] &lt; observations[1]:
                action = tf.random.uniform([1], 1, 2, dtype=tf.int32)
            else:
                action = tf.random.uniform([1], 0, 1, dtype=tf.int32)
        elif policy[0] == &#34;all&#34;:
            action = tf.random.uniform([1], 1, 2, dtype=tf.int32)
        else:
            if type(policy[0] == &#39;str&#39;):
                action = tf.random.uniform([1], 0, 2, dtype=tf.int32)

        return action

    def get_state_info(self) -&gt; dict:
        &#34;&#34;&#34;
        :return: the trip count and reward status of the driver
        &#34;&#34;&#34;
        return {
            &#34;goal_achieved&#34;: self.weekly_reward_reached,
            &#34;trip_count&#34;: self.weekly_trip_count
        }</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="RideSimulator.Driver.Driver.accept_ride"><code class="name flex">
<span>def <span class="ident">accept_ride</span></span>(<span>self, policy, observations) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>For now the driver randomly accepts or reject the trip.
The reinforcement learning model will be exposed to this function and the state will be passed to make the
decision.</p>
<p>:return: the decision of the driver</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def accept_ride(self, policy, observations) -&gt; bool:
    &#34;&#34;&#34;
    For now the driver randomly accepts or reject the trip.
    The reinforcement learning model will be exposed to this function and the state will be passed to make the
    decision.

    :return: the decision of the driver
    &#34;&#34;&#34;
    driver_policy = policy[self.id % len(policy)]
    # TODO - check before commit
    # previous reward shouldn&#39;t matter when decision making, so leave reward = 0?
    # observation_ts = ts.transition(np.array(observations, dtype=np.float32), reward=0.0, discount=1.0)
    # observation_ts = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]),
    #                              tf.convert_to_tensor(np.array([observations], dtype=np.float32), dtype=tf.float32))
    # logging.debug(observation_ts)
    # action = driver_policy.action(observation_ts)
    #print(&#34;obs&#34;, observations, &#34;act&#34;, action)

    if policy[0] == &#34;random&#34;:
        action = tf.random.uniform([1], 0, 2, dtype=tf.int32)

    elif policy[0] == &#34;positive&#34;:
        if observations[0] &lt; observations[1]:
            action = tf.random.uniform([1], 1, 2, dtype=tf.int32)
        else:
            action = tf.random.uniform([1], 0, 1, dtype=tf.int32)
    elif policy[0] == &#34;all&#34;:
        action = tf.random.uniform([1], 1, 2, dtype=tf.int32)
    else:
        if type(policy[0] == &#39;str&#39;):
            action = tf.random.uniform([1], 0, 2, dtype=tf.int32)

    return action</code></pre>
</details>
</dd>
<dt id="RideSimulator.Driver.Driver.get_state_info"><code class="name flex">
<span>def <span class="ident">get_state_info</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>:return: the trip count and reward status of the driver</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_state_info(self) -&gt; dict:
    &#34;&#34;&#34;
    :return: the trip count and reward status of the driver
    &#34;&#34;&#34;
    return {
        &#34;goal_achieved&#34;: self.weekly_reward_reached,
        &#34;trip_count&#34;: self.weekly_trip_count
    }</code></pre>
</details>
</dd>
<dt id="RideSimulator.Driver.Driver.reset_weekly_info"><code class="name flex">
<span>def <span class="ident">reset_weekly_info</span></span>(<span>self, new_target, new_reward)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_weekly_info(self, new_target, new_reward):
    self.weekly_trip_count = 0
    self.weekly_reward_reached = False
    # self.total_reward = 0
    self.weekly_target = new_target
    self.reward_amount = new_reward</code></pre>
</details>
</dd>
<dt id="RideSimulator.Driver.Driver.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self):
    while True:
        try:
            if self.in_trip:
                yield self.env.timeout(self.trip_duration)
                logging.debug(f&#34;Driver {self.id} finished trip&#34;)
            else:
                yield self.env.timeout(float(&#39;inf&#39;))
        except simpy.Interrupt:
            continue</code></pre>
</details>
</dd>
<dt id="RideSimulator.Driver.Driver.start_trip"><code class="name flex">
<span>def <span class="ident">start_trip</span></span>(<span>self, duration: int, location: numpy.ndarray, trip: <a title="RideSimulator.Trip.Trip" href="Trip.html#RideSimulator.Trip.Trip">Trip</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>When the driver starts the trip, his 'total_trip_count' and 'weekly_reward_reached' values are updated.
If the driver has achieved the weekly goal, an additional amount of reward is added.</p>
<p>:param duration: trip duration
:param location: drop location of the trip
:param trip: trip object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_trip(self, duration: int, location: np.ndarray, trip: Trip):
    &#34;&#34;&#34;
    When the driver starts the trip, his &#39;total_trip_count&#39; and &#39;weekly_reward_reached&#39; values are updated.
    If the driver has achieved the weekly goal, an additional amount of reward is added.

    :param duration: trip duration
    :param location: drop location of the trip
    :param trip: trip object
    &#34;&#34;&#34;
    self.in_trip = True
    self.trip = trip
    self.trip_duration = duration
    self.weekly_trip_count += 1
    self.location = location
    add_reward = False
    logging.debug(f&#34;driver {self.id} location updated  to {self.location}&#34;)

    # Record rewards and distances.
    self.total_reward += trip.get_net_reward()

    if (not self.weekly_reward_reached) and (self.weekly_trip_count &gt;= self.weekly_target):
        self.weekly_reward_reached = True
        self.total_reward += self.reward_amount
        add_reward = True
        logging.debug(f&#34;driver {self.id} has achieved his weekly reward&#34;)
    self.action.interrupt()

    return add_reward</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="RideSimulator" href="index.html">RideSimulator</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="RideSimulator.Driver.Driver" href="#RideSimulator.Driver.Driver">Driver</a></code></h4>
<ul class="">
<li><code><a title="RideSimulator.Driver.Driver.accept_ride" href="#RideSimulator.Driver.Driver.accept_ride">accept_ride</a></code></li>
<li><code><a title="RideSimulator.Driver.Driver.get_state_info" href="#RideSimulator.Driver.Driver.get_state_info">get_state_info</a></code></li>
<li><code><a title="RideSimulator.Driver.Driver.reset_weekly_info" href="#RideSimulator.Driver.Driver.reset_weekly_info">reset_weekly_info</a></code></li>
<li><code><a title="RideSimulator.Driver.Driver.run" href="#RideSimulator.Driver.Driver.run">run</a></code></li>
<li><code><a title="RideSimulator.Driver.Driver.start_trip" href="#RideSimulator.Driver.Driver.start_trip">start_trip</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>